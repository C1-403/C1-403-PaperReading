# 论文信息
- 时间：2021
- 期刊：CVPR
- 网络名称：MAE 
- 意义：BERT的CV版
- 作者：Kaiming He∗,† , Xinlei Chen∗ , Saining Xie,  Yanghao Li,  Piotr Dollar,  Ross Girshick, ∗equal technical contribution, †project lead; Facebook AI Research (FAIR)
- 实验环境：
- 数据集：
# 论文知识补充
- 做的模型很快叫`efficient`
- 做的模型很大叫`scalable`
- `Auto`在这篇论文中不是“自动”，而是“自”，如自回归 
# 一、解决的问题
- 在图片里加入噪声，然后让模型学习如何去噪
- Transformer和CNN的区别：
  1. 结构不一样，卷积难以加入掩码，无法区分边界
  2. 信息密度不一样，作者通过把高比率的块去掉，减少冗余
  3. 自编码器的解码器：CV还原较低层次的内容（像素），因此不能像NLP一样使用一个简单的全连接层，而是使用卷积或者其他层
# 二、做出的创新
1. 将BERT方法拓展到了CV中，通过MASK做完形填空
2. 将图片分割成多个小块，然后进行随机掩码，在进行打乱，再让模型进行复原
3. 编码器和解码器不对成，即看到的东西不一样
# 三、设计的模型
![MAE architecture](../pictures/MAE%20architecture.png)

- 图片切成小块
- 随机盖住一些块
- 把没改住的块拉成向量送入编码器
- 编码器结果中加入盖住的块，补充位置信息，送入解码器
- 让解码器把图片进行还原
- 通过这样的方式可以降低计算量
# 四、实验结果
- 遮盖的区域面积越小，还原的效果越好
## 1、比之前模型的优势

## 2、有优势的原因

## 3、改进空间

# 五、结论

## 1、模型是否解决了目标问题

## 2、模型是否遗留了问题

## 3、模型是否引入了新的问题

# 六、代码

# 读者角度（挖掘文章中没有提到的）：
1. 总结文章发现问题的思路
- 一一引出BERT->CV可能出现的问题，一一解答后，引出了模型
2. 总结文章改进的思想
3. 总结文章还存在或者可以改进的问题
4. 提出对模型参数和细节的一些思考和讨论