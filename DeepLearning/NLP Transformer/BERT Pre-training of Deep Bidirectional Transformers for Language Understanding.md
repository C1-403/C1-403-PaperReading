# 论文信息
- 时间：2018
- 期刊：ACL
- 网络名称： BERT
- 意义：Transformer一统NLP的开始
- 作者：Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova; Google AI Language
- 实验环境：
- 数据集：
# 一、解决的问题
1. 在NLP中，BERT之前，始终没有一个大的、深的神经网络可以在很大的数据集上训练好之后，帮助一大批的NLP任务
2. 参考ELMo（芝麻街里的人物），他们凑了一个BERT（芝麻街里的人物），Bideirection Encoder Representations from Transformer
3. 单向分析句子是非常不合理的，也不符合部分事实
# 二、做出的创新
1. 与ELMo和GPT不同，相比于GPT只用左侧信息，BERT用了右侧和左侧的信息去做预测，所以它是双向的；ELMo用的是基于RNN的架构，BERT用的是Transformer，所以ELMo做下游任务的时候，要对结构进行一些调整，而BERT就相对简单很多，只需要改对象就可以了
2. 受1953年一篇close的论文影响，使用了带掩码的语言模型，随机将句子中的词进行掩码
4. 还训练了输入两个句子，让模型判断两个句子是否相关，或者是随机的两个句子
5. BERT是第一个基于微调的模型，在词源层面和句子层面任务都很好
6. 重点就是把前人的工作，拓展到了深的、双向结构上
# 三、设计的模型

# 四、实验结果

## 1、比之前模型的优势

## 2、有优势的原因

## 3、改进空间

# 五、结论

## 1、模型是否解决了目标问题

## 2、模型是否遗留了问题

## 3、模型是否引入了新的问题

# 六、代码

# 读者角度（挖掘文章中没有提到的）：
1. 总结文章发现问题的思路
2. 总结文章改进的思想
3. 总结文章还存在或者可以改进的问题
4. 提出对模型参数和细节的一些思考和讨论